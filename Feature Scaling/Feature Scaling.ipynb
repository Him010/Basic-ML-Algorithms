{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling is a part of pre processing that we do before assigning any classifier to the dataset i.e. it is the first thing we do and then we do things with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1.,-1.,2.],\n",
    "              [2.,0.,0.],\n",
    "              [0.,1.,-1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)    # it centers it around the mean that means it basically makes the mean to be zero.\n",
    "                          # secondly it makes the variance to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)   # looking at mean along axis = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)    # looking at standard deviation values along axis = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the training data is scaled alone and the testing data is scaled alone too i.e. both of them are not scaled together\n",
    "then it is going to lead to huge problems in our classification and regression questions because the parameters for scaling\n",
    "i.e. the parameters used for subtracting and dividing would be different in both the training and the testing cases\n",
    "so if both the training and the testing data is different then make them into a single array and then scale the data.\n",
    "after that you can again break the data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the above choice is not wise as when we are building an application so our training data will be with us but our testing\n",
    "data will come when the customer is actually on the website after the product has been deployed. so it won't be possible to\n",
    "combine the training data and the testing data then and their and then retrain the training data. so what should be done is\n",
    "that the we should store the parameters used in feature scaling the training data and then use the same parameters when a user\n",
    "accessess the product and give results according to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so the below code is use to that same thing that we just talked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()       # like a classifier like in linear regression and logistic regression and it\n",
    "                                              # return an object\n",
    "scaler.fit(X)               # fits the training data X on the scaler object\n",
    "\n",
    "scaler.transform(X)         # transforms any data only according to the parameters generated while using fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.22474487, -0.26726124]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [[1,1,0]]\n",
    "scaler.transform(X_test)    # it is looking for an array of array format so even if you are just giving one testing data you\n",
    "                            # need to give it in array of array format i.e. 2-D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so the way we have StandardScaler() so we have MinMaxScaler() and MaxAbsoluteScaler() as well but most of the time we'll be using StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### these three scalers do take parameters like StandardScaler() takes parameters like mean and std i.e. what you want these values to be. by default mean is 0 and std is 1\n",
    "#### similarly in MinMaxScaler() we can pass the minimum and the maximum value. by default the minimum and the maximum values are 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[0,3],[9,4],[2,7],[1,1]]\n",
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.879529301279849\n"
     ]
    }
   ],
   "source": [
    "first=scaler.transform(data)\n",
    "first.sum()\n",
    "second=scaler.transform([[2,13],[1,4],[10,7],[1,9]])\n",
    "print(second.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
